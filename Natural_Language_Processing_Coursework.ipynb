{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzZc8SvGNq4u14QS1nottf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScottHay14/Natural-Language-Processing-Coursework/blob/main/Natural_Language_Processing_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 - Dataset"
      ],
      "metadata": {
        "id": "e1FbPlkayA5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Drug Reviews dataset from Druglib.com is a collection of patient reviews on specific drugs along with the related conditions. The dataset is broken up into these 9 variables.\n",
        "<br>\n",
        "<br>reviewID\n",
        "<br>urlDrugName\n",
        "<br>rating\n",
        "<br>effectiveness\n",
        "<br>sideEffects\n",
        "<br>condition\n",
        "<br>benefitsReview\n",
        "<br>sideEffectsReview\n",
        "<br>commentsReview\n",
        "<br>\n",
        "<br>\n",
        "The task going to be performed in my classwork is text classification with the goal of predicting drug effectivness ratings from the patients reviews. The effectiveness variable is categorical and contains 5 options of effectiveness these being Highly Effective, Considerably Effective, Moderately Effective, Marginally Effective, Ineffective.\n",
        "\n"
      ],
      "metadata": {
        "id": "YeUDDzlxjI9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab deletes the folder and need to have the Data folder with the dataset. Running this gets it from the github and copies Data folder to /content/Data\n",
        "!git clone https://github.com/ScottHay14/Natural-Language-Processing-Coursework\n",
        "!cp -r /content/Natural-Language-Processing-Coursework/Data /content/Data"
      ],
      "metadata": {
        "id": "mO-ToHImP2Zw",
        "outputId": "e280d462-eddf-4486-ec0d-e30e07937a38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Natural-Language-Processing-Coursework'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/41)\u001b[K\rremote: Counting objects:   4% (2/41)\u001b[K\rremote: Counting objects:   7% (3/41)\u001b[K\rremote: Counting objects:   9% (4/41)\u001b[K\rremote: Counting objects:  12% (5/41)\u001b[K\rremote: Counting objects:  14% (6/41)\u001b[K\rremote: Counting objects:  17% (7/41)\u001b[K\rremote: Counting objects:  19% (8/41)\u001b[K\rremote: Counting objects:  21% (9/41)\u001b[K\rremote: Counting objects:  24% (10/41)\u001b[K\rremote: Counting objects:  26% (11/41)\u001b[K\rremote: Counting objects:  29% (12/41)\u001b[K\rremote: Counting objects:  31% (13/41)\u001b[K\rremote: Counting objects:  34% (14/41)\u001b[K\rremote: Counting objects:  36% (15/41)\u001b[K\rremote: Counting objects:  39% (16/41)\u001b[K\rremote: Counting objects:  41% (17/41)\u001b[K\rremote: Counting objects:  43% (18/41)\u001b[K\rremote: Counting objects:  46% (19/41)\u001b[K\rremote: Counting objects:  48% (20/41)\u001b[K\rremote: Counting objects:  51% (21/41)\u001b[K\rremote: Counting objects:  53% (22/41)\u001b[K\rremote: Counting objects:  56% (23/41)\u001b[K\rremote: Counting objects:  58% (24/41)\u001b[K\rremote: Counting objects:  60% (25/41)\u001b[K\rremote: Counting objects:  63% (26/41)\u001b[K\rremote: Counting objects:  65% (27/41)\u001b[K\rremote: Counting objects:  68% (28/41)\u001b[K\rremote: Counting objects:  70% (29/41)\u001b[K\rremote: Counting objects:  73% (30/41)\u001b[K\rremote: Counting objects:  75% (31/41)\u001b[K\rremote: Counting objects:  78% (32/41)\u001b[K\rremote: Counting objects:  80% (33/41)\u001b[K\rremote: Counting objects:  82% (34/41)\u001b[K\rremote: Counting objects:  85% (35/41)\u001b[K\rremote: Counting objects:  87% (36/41)\u001b[K\rremote: Counting objects:  90% (37/41)\u001b[K\rremote: Counting objects:  92% (38/41)\u001b[K\rremote: Counting objects:  95% (39/41)\u001b[K\rremote: Counting objects:  97% (40/41)\u001b[K\rremote: Counting objects: 100% (41/41)\u001b[K\rremote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 41 (delta 12), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (41/41), 1.08 MiB | 5.54 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2sX7fX0ilHsW"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Data and combining the test and train dataset into one dataframe\n",
        "test_data = \"/content/Data/drugLibTest_raw.tsv\"\n",
        "train_data = \"/content/Data/drugLibTrain_raw.tsv\"\n",
        "\n",
        "test_df = pd.read_csv(test_data, delimiter=\"\\t\")\n",
        "train_df = pd.read_csv(train_data, delimiter=\"\\t\")\n",
        "\n",
        "df = pd.concat([test_df, train_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "J5ziK43DuBzM"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploring Data\n",
        "print(df.head()) # Just printing first rows to see if loaded correctly\n",
        "\n",
        "\n",
        "print(\"\\nClass distribution\")\n",
        "print(df[\"effectiveness\"].value_counts()) # Shows a large class imbalance\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvpt0etIu1hL",
        "outputId": "7bfe44a1-bae3-427a-f0c0-41784c7c8a9c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 urlDrugName  rating           effectiveness  \\\n",
            "0        1366      biaxin       9  Considerably Effective   \n",
            "1        3724    lamictal       9        Highly Effective   \n",
            "2        3824    depakene       4    Moderately Effective   \n",
            "3         969     sarafem      10        Highly Effective   \n",
            "4         696    accutane      10        Highly Effective   \n",
            "\n",
            "           sideEffects           condition  \\\n",
            "0    Mild Side Effects     sinus infection   \n",
            "1    Mild Side Effects    bipolar disorder   \n",
            "2  Severe Side Effects    bipolar disorder   \n",
            "3      No Side Effects  bi-polar / anxiety   \n",
            "4    Mild Side Effects        nodular acne   \n",
            "\n",
            "                                      benefitsReview  \\\n",
            "0  The antibiotic may have destroyed bacteria cau...   \n",
            "1  Lamictal stabilized my serious mood swings. On...   \n",
            "2  Initial benefits were comparable to the brand ...   \n",
            "3  It controlls my mood swings. It helps me think...   \n",
            "4  Within one week of treatment superficial acne ...   \n",
            "\n",
            "                                   sideEffectsReview  \\\n",
            "0                      Some back pain, some nauseau.   \n",
            "1  Drowsiness, a bit of mental numbness. If you t...   \n",
            "2  Depakene has a very thin coating, which caused...   \n",
            "3            I didnt really notice any side effects.   \n",
            "4  Side effects included moderate to severe dry s...   \n",
            "\n",
            "                                      commentsReview  \n",
            "0  Took the antibiotics for 14 days. Sinus infect...  \n",
            "1  Severe mood swings between hypomania and depre...  \n",
            "2  Depakote was prescribed to me by a Kaiser psyc...  \n",
            "3  This drug may not be for everyone but its wond...  \n",
            "4  Drug was taken in gelatin tablet at 0.5 mg per...  \n",
            "\n",
            "Class distribution\n",
            "effectiveness\n",
            "Highly Effective          1741\n",
            "Considerably Effective    1238\n",
            "Moderately Effective       572\n",
            "Ineffective                329\n",
            "Marginally Effective       263\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the 3 review categories into one (benefitsReview, sideEffectsReview, commentsReview) for both the training dataset and the testing dataset\n",
        "\n",
        "# Train dataset combined first\n",
        "train_df[\"combined_review\"] = train_df[\"benefitsReview\"].fillna(\"\").astype(str) + \"\\n\\n\" + train_df[\"sideEffectsReview\"].fillna(\"\").astype(str) + \"\\n\\n\" +  train_df[\"commentsReview\"].fillna(\"\").astype(str)\n",
        "x_train = train_df[\"combined_review\"].to_numpy()\n",
        "y_train = train_df[\"effectiveness\"].to_numpy()\n",
        "print(\"Train dataset example\")\n",
        "print(x_train[0][:1000])\n",
        "print(y_train[0])\n",
        "print(\"\\n\")\n",
        "\n",
        "# Test dataset combined after\n",
        "test_df[\"combined_review\"] = test_df[\"benefitsReview\"].fillna(\"\").astype(str) + \"\\n\\n\" + test_df[\"sideEffectsReview\"].fillna(\"\").astype(str) + \"\\n\\n\" +  test_df[\"commentsReview\"].fillna(\"\").astype(str)\n",
        "x_test = test_df[\"combined_review\"].to_numpy()\n",
        "y_test = test_df[\"effectiveness\"].to_numpy()\n",
        "print(\"Test dataset example\")\n",
        "print(x_test[0][:1000])\n",
        "print(y_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5hBUfZtWxKS",
        "outputId": "4375fc8a-a66f-4fe3-9601-f5c4a63663cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset example\n",
            "slowed the progression of left ventricular dysfunction into overt heart failure \r\r\n",
            "alone or with other agents in the managment of hypertension \r\r\n",
            "mangagement of congestive heart failur\n",
            "\n",
            "cough, hypotension , proteinuria, impotence , renal failure , angina pectoris , tachycardia , eosinophilic pneumonitis, tastes disturbances , anusease anorecia , weakness fatigue insominca weakness\n",
            "\n",
            "monitor blood pressure , weight and asses for resolution of fluid\n",
            "Highly Effective\n",
            "\n",
            "\n",
            "Test dataset example\n",
            "The antibiotic may have destroyed bacteria causing my sinus infection.  But it may also have been caused by a virus, so its hard to say.\n",
            "\n",
            "Some back pain, some nauseau.\n",
            "\n",
            "Took the antibiotics for 14 days. Sinus infection was gone after the 6th day.\n",
            "Considerably Effective\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def prep(X):\n",
        "  prep_text = []\n",
        "  for x in X:\n",
        "    token_text = word_tokenize(x)\n",
        "    normd_text = [token.lower() for token in token_text if token.isalpha()]\n",
        "    swr_text = [token for token in normd_text if token not in stopwords.words(\"english\")]\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    prep_text += [[stemmer.stem(word) for word in swr_text]]\n",
        "  prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "  return prep_sentences\n",
        "\n",
        "prep_x_train = prep(x_train)\n",
        "prep_x_test = prep(x_test)\n",
        "\n",
        "print(\"Preprocessed working for train dataset\")\n",
        "print(prep_x_train[0][:1000])\n",
        "\n",
        "print(\"Preprocessed working for test dataset\")\n",
        "print(prep_x_test[0][:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOP-h3hwbMZy",
        "outputId": "43b819a1-4b43-4921-8a74-aabd8ba425e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed working for train dataset\n",
            "slow progress left ventricular dysfunct overt heart failur alon agent manag hypertens mangag congest heart failur cough hypotens proteinuria impot renal failur angina pectori tachycardia eosinophil pneumon tast disturb anuseas anorecia weak fatigu insominca weak monitor blood pressur weight ass resolut fluid\n",
            "Preprocessed working for test dataset\n",
            "antibiot may destroy bacteria caus sinus infect may also caus virus hard say back pain nauseau took antibiot day sinus infect gone day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 - Representation Learning"
      ],
      "metadata": {
        "id": "UXJvNSCPyG8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term frequency-inverse document frequency (tf-idf) is a way to measure the importance of a word within a document or a collection of documents also known as a corpus. For the drug dataset the document would be the combined review. TF has a formula of\n",
        "\n",
        "TF(t,d) = Number of times term t appears in document d / Total number of terms in document\n",
        "So if the word \"slowed\" for example appeared once and the combined review had a total of 75 words TF=1/75=0.013\n",
        "The next part is IDF which will measure how rare a word is across a corpus. It has the formula\n",
        "\n",
        "IDF(t,d) = log(Total number of doucments in the corpus N / Number of documents containing term t)\n",
        "So for a common word like drug it would have a lower idf.\n",
        "\n",
        "The final part is just multiplying so TF-IDF = TF * IDF this results in words getting a higher TF-IDF if it appears a lot in this context a review but is not common in the corpus."
      ],
      "metadata": {
        "id": "jDToZ8ibigc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "te-cHNWijwyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Representation Learning - TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = tfidf.fit_transform(prep_x_train)\n",
        "x_test_tfidf = tfidf.transform(prep_x_test)\n",
        "\n",
        "# Data Exploration\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "print(f\"Sample {list(feature_names)[:15]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8uIQ_5F_4Qv",
        "outputId": "c7ae489f-957b-463f-cfaa-21e186281b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample ['aarm', 'aarp', 'abait', 'abandon', 'abat', 'abbout', 'abbsess', 'abcess', 'abdo', 'abdomen', 'abdomin', 'aberr', 'abfter', 'abil', 'abilifi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3 - Algorithms"
      ],
      "metadata": {
        "id": "mDHwhU8-yPnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Support Vector Classification\n",
        "\n",
        "\n",
        "## Multinomial Naive Bayes\n",
        "Multinomial navie bayes is a variation based of Navie Bayes algorithm and is mainly used for text classification."
      ],
      "metadata": {
        "id": "mRUHHJFMFT74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Initialize score lists for multiple metrics\n",
        "svm_acc_scores = []\n",
        "svm_f1_macro_scores = []\n",
        "mnb_acc_scores = []\n",
        "mnb_f1_macro_scores = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(prep_x_train, y_train_encoded), 1):\n",
        "    print(f\"\\nFold {fold}/10\")\n",
        "\n",
        "    x_train_fold = [prep_x_train[i] for i in train_idx]\n",
        "    x_val_fold = [prep_x_train[i] for i in val_idx]\n",
        "    y_train_fold = y_train_encoded[train_idx]\n",
        "    y_val_fold = y_train_encoded[val_idx]\n",
        "\n",
        "    tfidf_fold = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2,\n",
        "        max_df=0.90,\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    x_train_tfidf = tfidf_fold.fit_transform(x_train_fold)\n",
        "    x_val_tfidf = tfidf_fold.transform(x_val_fold)\n",
        "\n",
        "    svm = LinearSVC(\n",
        "        class_weight=\"balanced\", # balanced mode uses value of y to adjust weights\n",
        "        max_iter=1000,\n",
        "        C=1.0\n",
        "    )\n",
        "\n",
        "    svm.fit(x_train_tfidf, y_train_fold)\n",
        "    svm_pred = svm.predict(x_val_tfidf)\n",
        "\n",
        "    svm_acc = accuracy_score(y_val_fold, svm_pred)\n",
        "    svm_f1_macro = f1_score(y_val_fold, svm_pred, average='macro')\n",
        "\n",
        "    svm_acc_scores.append(svm_acc)\n",
        "    svm_f1_macro_scores.append(svm_f1_macro)\n",
        "\n",
        "    print(f\"  LinearSVC Accuracy: {svm_acc:.4f}, F1-macro: {svm_f1_macro:.4f}\")\n",
        "\n",
        "    mnb = MultinomialNB(alpha=1.0)\n",
        "    mnb.fit(x_train_tfidf, y_train_fold)\n",
        "    mnb_pred = mnb.predict(x_val_tfidf)\n",
        "\n",
        "    mnb_acc = accuracy_score(y_val_fold, mnb_pred)\n",
        "    mnb_f1_macro = f1_score(y_val_fold, mnb_pred, average='macro')\n",
        "\n",
        "    mnb_acc_scores.append(mnb_acc)\n",
        "    mnb_f1_macro_scores.append(mnb_f1_macro)\n",
        "\n",
        "    print(f\"  MultinomialNB Accuracy: {mnb_acc:.4f}, F1-macro: {mnb_f1_macro:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66fFR842LKDO",
        "outputId": "21c0daec-5427-4273-8aac-a82a3c57588e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/10\n",
            "  LinearSVC Accuracy: 0.4695, F1-macro: 0.3449\n",
            "  MultinomialNB Accuracy: 0.4598, F1-macro: 0.1707\n",
            "\n",
            "Fold 2/10\n",
            "  LinearSVC Accuracy: 0.4630, F1-macro: 0.3886\n",
            "  MultinomialNB Accuracy: 0.4534, F1-macro: 0.1640\n",
            "\n",
            "Fold 3/10\n",
            "  LinearSVC Accuracy: 0.4116, F1-macro: 0.3186\n",
            "  MultinomialNB Accuracy: 0.4469, F1-macro: 0.1598\n",
            "\n",
            "Fold 4/10\n",
            "  LinearSVC Accuracy: 0.4598, F1-macro: 0.3611\n",
            "  MultinomialNB Accuracy: 0.4469, F1-macro: 0.1573\n",
            "\n",
            "Fold 5/10\n",
            "  LinearSVC Accuracy: 0.4662, F1-macro: 0.4035\n",
            "  MultinomialNB Accuracy: 0.4373, F1-macro: 0.1517\n",
            "\n",
            "Fold 6/10\n",
            "  LinearSVC Accuracy: 0.4148, F1-macro: 0.2935\n",
            "  MultinomialNB Accuracy: 0.4405, F1-macro: 0.1502\n",
            "\n",
            "Fold 7/10\n",
            "  LinearSVC Accuracy: 0.4534, F1-macro: 0.3648\n",
            "  MultinomialNB Accuracy: 0.4277, F1-macro: 0.1539\n",
            "\n",
            "Fold 8/10\n",
            "  LinearSVC Accuracy: 0.4484, F1-macro: 0.3659\n",
            "  MultinomialNB Accuracy: 0.4613, F1-macro: 0.1688\n",
            "\n",
            "Fold 9/10\n",
            "  LinearSVC Accuracy: 0.4774, F1-macro: 0.3632\n",
            "  MultinomialNB Accuracy: 0.4355, F1-macro: 0.1429\n",
            "\n",
            "Fold 10/10\n",
            "  LinearSVC Accuracy: 0.4839, F1-macro: 0.3901\n",
            "  MultinomialNB Accuracy: 0.4484, F1-macro: 0.1548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4 - Evaluation"
      ],
      "metadata": {
        "id": "5SXJoPQzyVUR"
      }
    }
  ]
}