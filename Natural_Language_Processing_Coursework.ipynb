{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOliivbI80fcWrqkCDeH9vp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScottHay14/Natural-Language-Processing-Coursework/blob/main/Natural_Language_Processing_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 - Dataset"
      ],
      "metadata": {
        "id": "e1FbPlkayA5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Drug Reviews dataset from Druglib.com is a collection of patient reviews on specific drugs along with the related conditions. The dataset is broken up into these 9 variables.\n",
        "<br>\n",
        "<br>reviewID\n",
        "<br>urlDrugName\n",
        "<br>rating\n",
        "<br>effectiveness\n",
        "<br>sideEffects\n",
        "<br>condition\n",
        "<br>benefitsReview\n",
        "<br>sideEffectsReview\n",
        "<br>commentsReview\n",
        "<br>\n",
        "<br>\n",
        "The task going to be performed in my classwork is text classification with the goal of predicting drug effectivness ratings from the patients reviews. The effectiveness variable is categorical and contains 5 options of effectiveness these being Highly Effective, Considerably Effective, Moderately Effective, Marginally Effective, Ineffective.\n",
        "\n"
      ],
      "metadata": {
        "id": "YeUDDzlxjI9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2sX7fX0ilHsW"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Data and combining the test and train dataset into one dataframe\n",
        "test_data = \"/content/Data/drugLibTest_raw.tsv\"\n",
        "train_data = \"/content/Data/drugLibTrain_raw.tsv\"\n",
        "\n",
        "test_df = pd.read_csv(test_data, delimiter=\"\\t\")\n",
        "train_df = pd.read_csv(train_data, delimiter=\"\\t\")\n",
        "\n",
        "df = pd.concat([test_df, train_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "J5ziK43DuBzM"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploring Data\n",
        "print(df.head()) # Just printing first rows to see if loaded correctly\n",
        "\n",
        "\n",
        "print(\"\\nClass distribution\")\n",
        "print(df[\"effectiveness\"].value_counts()) # Shows a large class imbalance\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvpt0etIu1hL",
        "outputId": "b6a995ac-d7b4-4c9f-8bdd-c2b049dbb091"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 urlDrugName  rating           effectiveness  \\\n",
            "0        1366      biaxin       9  Considerably Effective   \n",
            "1        3724    lamictal       9        Highly Effective   \n",
            "2        3824    depakene       4    Moderately Effective   \n",
            "3         969     sarafem      10        Highly Effective   \n",
            "4         696    accutane      10        Highly Effective   \n",
            "\n",
            "           sideEffects           condition  \\\n",
            "0    Mild Side Effects     sinus infection   \n",
            "1    Mild Side Effects    bipolar disorder   \n",
            "2  Severe Side Effects    bipolar disorder   \n",
            "3      No Side Effects  bi-polar / anxiety   \n",
            "4    Mild Side Effects        nodular acne   \n",
            "\n",
            "                                      benefitsReview  \\\n",
            "0  The antibiotic may have destroyed bacteria cau...   \n",
            "1  Lamictal stabilized my serious mood swings. On...   \n",
            "2  Initial benefits were comparable to the brand ...   \n",
            "3  It controlls my mood swings. It helps me think...   \n",
            "4  Within one week of treatment superficial acne ...   \n",
            "\n",
            "                                   sideEffectsReview  \\\n",
            "0                      Some back pain, some nauseau.   \n",
            "1  Drowsiness, a bit of mental numbness. If you t...   \n",
            "2  Depakene has a very thin coating, which caused...   \n",
            "3            I didnt really notice any side effects.   \n",
            "4  Side effects included moderate to severe dry s...   \n",
            "\n",
            "                                      commentsReview  \n",
            "0  Took the antibiotics for 14 days. Sinus infect...  \n",
            "1  Severe mood swings between hypomania and depre...  \n",
            "2  Depakote was prescribed to me by a Kaiser psyc...  \n",
            "3  This drug may not be for everyone but its wond...  \n",
            "4  Drug was taken in gelatin tablet at 0.5 mg per...  \n",
            "\n",
            "Class distribution\n",
            "effectiveness\n",
            "Highly Effective          1741\n",
            "Considerably Effective    1238\n",
            "Moderately Effective       572\n",
            "Ineffective                329\n",
            "Marginally Effective       263\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the 3 review categories into one (benefitsReview, sideEffectsReview, commentsReview) for both the training dataset and the testing dataset\n",
        "\n",
        "# Train dataset combined first\n",
        "train_df[\"combined_review\"] = train_df[\"benefitsReview\"].fillna(\"\").astype(str) + \"\\n\\n\" + train_df[\"sideEffectsReview\"].fillna(\"\").astype(str) + \"\\n\\n\" +  train_df[\"commentsReview\"].fillna(\"\").astype(str)\n",
        "x_train = train_df[\"combined_review\"].to_numpy()\n",
        "y_train = train_df[\"effectiveness\"].to_numpy()\n",
        "print(\"Train dataset example\")\n",
        "print(x_train[0][:1000])\n",
        "print(y_train[0])\n",
        "print(\"\\n\")\n",
        "\n",
        "# Test dataset combined after\n",
        "test_df[\"combined_review\"] = test_df[\"benefitsReview\"].fillna(\"\").astype(str) + \"\\n\\n\" + test_df[\"sideEffectsReview\"].fillna(\"\").astype(str) + \"\\n\\n\" +  test_df[\"commentsReview\"].fillna(\"\").astype(str)\n",
        "x_test = test_df[\"combined_review\"].to_numpy()\n",
        "y_test = test_df[\"effectiveness\"].to_numpy()\n",
        "print(\"Test dataset example\")\n",
        "print(x_test[0][:1000])\n",
        "print(y_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5hBUfZtWxKS",
        "outputId": "4375fc8a-a66f-4fe3-9601-f5c4a63663cf"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset example\n",
            "slowed the progression of left ventricular dysfunction into overt heart failure \r\r\n",
            "alone or with other agents in the managment of hypertension \r\r\n",
            "mangagement of congestive heart failur\n",
            "\n",
            "cough, hypotension , proteinuria, impotence , renal failure , angina pectoris , tachycardia , eosinophilic pneumonitis, tastes disturbances , anusease anorecia , weakness fatigue insominca weakness\n",
            "\n",
            "monitor blood pressure , weight and asses for resolution of fluid\n",
            "Highly Effective\n",
            "\n",
            "\n",
            "Test dataset example\n",
            "The antibiotic may have destroyed bacteria causing my sinus infection.  But it may also have been caused by a virus, so its hard to say.\n",
            "\n",
            "Some back pain, some nauseau.\n",
            "\n",
            "Took the antibiotics for 14 days. Sinus infection was gone after the 6th day.\n",
            "Considerably Effective\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def prep(X):\n",
        "  prep_text = []\n",
        "  for x in X:\n",
        "    token_text = word_tokenize(x)\n",
        "    normd_text = [token.lower() for token in token_text if token.isalpha()]\n",
        "    swr_text = [token for token in normd_text if token not in stopwords.words(\"english\")]\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    prep_text += [[stemmer.stem(word) for word in swr_text]]\n",
        "  prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "  return prep_sentences\n",
        "\n",
        "prep_x_train = prep(x_train)\n",
        "prep_x_test = prep(x_test)\n",
        "\n",
        "print(\"Preprocessed working for train dataset\")\n",
        "print(prep_x_train[0][:1000])\n",
        "\n",
        "print(\"Preprocessed working for test dataset\")\n",
        "print(prep_x_test[0][:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOP-h3hwbMZy",
        "outputId": "43b819a1-4b43-4921-8a74-aabd8ba425e3"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed working for train dataset\n",
            "slow progress left ventricular dysfunct overt heart failur alon agent manag hypertens mangag congest heart failur cough hypotens proteinuria impot renal failur angina pectori tachycardia eosinophil pneumon tast disturb anuseas anorecia weak fatigu insominca weak monitor blood pressur weight ass resolut fluid\n",
            "Preprocessed working for test dataset\n",
            "antibiot may destroy bacteria caus sinus infect may also caus virus hard say back pain nauseau took antibiot day sinus infect gone day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 - Representation Learning"
      ],
      "metadata": {
        "id": "UXJvNSCPyG8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "jDToZ8ibigc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install to get the Word2Vec to work\n",
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te-cHNWijwyN",
        "outputId": "30582256-b534-4e8f-98d7-c7af59190bb7"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First Representation Learning - Word2Vec\n",
        "def word2vec_rep(sentence, w2v_model):\n",
        "  embs = [w2v_model.wv[word] for word in sentence if word in w2v_model.wv.index_to_key]\n",
        "  if len(embs) > 0:\n",
        "    sent_emb = np.mean(np.array(embs), axis=0)\n",
        "  else:\n",
        "    sent_emb = np.zeros(w2v_model.vector_size)\n",
        "  return sent_emb\n",
        "\n",
        "prep_x_train_token = [sentence.split() for sentence in prep_x_train]\n",
        "prep_x_test_token = [sentence.split() for sentence in prep_x_test]\n",
        "\n",
        "w2v_model = Word2Vec(vector_size=300, window=5, min_count=2, workers=4, sg=0)\n",
        "w2v_model.build_vocab(prep_x_train_token, update=None)\n",
        "\n",
        "w2v_model.train(\n",
        "    corpus_iterable=prep_x_train_token,\n",
        "    total_examples=len(prep_x_train_token),\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "print(f\"Vocabulary: {len(w2v_model.wv.index_to_key)}\")\n",
        "print(f\"Sample words: {list(w2v_model.wv.index_to_key)[:10]}\")\n",
        "\n",
        "x_train_representation = [word2vec_rep(instance, w2v_model) for instance in prep_x_train_token]\n",
        "x_test_representation = [word2vec_rep(instance, w2v_model) for instance in prep_x_test_token]\n",
        "\n",
        "x_train_representation = np.array(x_train_representation)\n",
        "x_test_representation = np.array(x_test_representation)\n",
        "\n",
        "\n",
        "# Data Exploration\n",
        "print(\"Words that are similar to 'Sick'\",w2v_model.wv.most_similar(\"sick\", topn=3))\n",
        "print(\"Words that are similar to 'Benefit'\",w2v_model.wv.most_similar(\"benefit\", topn=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXV1C96LiiDV",
        "outputId": "05037c96-99ba-414c-be95-06d61094e8a9"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: 4966\n",
            "Sample words: ['take', 'day', 'effect', 'side', 'time', 'drug', 'pain', 'medic', 'week', 'feel']\n",
            "Words that are similar to 'Sick' [('flu', 0.9324759244918823), ('throw', 0.9315365552902222), ('lighthead', 0.910933256149292)]\n",
            "Words that are similar to 'Benefit' [('outweigh', 0.9016796946525574), ('negat', 0.8599735498428345), ('result', 0.8280755281448364)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second Representation Learning - TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "x_train_tfidf = tfidf.fit_transform(prep_x_train)\n",
        "x_test_tfidf = tfidf.transform(prep_x_test)\n",
        "\n",
        "# Data Exploration\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "print(f\"Sample {list(feature_names)[:15]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8uIQ_5F_4Qv",
        "outputId": "da988031-a94b-4b96-80a2-c8374295fb44"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample ['aarm', 'aarp', 'abait', 'abandon', 'abat', 'abbout', 'abbsess', 'abcess', 'abdo', 'abdomen', 'abdomin', 'aberr', 'abfter', 'abil', 'abilifi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3 - Algorithms"
      ],
      "metadata": {
        "id": "mDHwhU8-yPnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "mRUHHJFMFT74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Initialize score lists for multiple metrics\n",
        "svm_acc_scores = []\n",
        "svm_f1_macro_scores = []\n",
        "mnb_acc_scores = []\n",
        "mnb_f1_macro_scores = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(prep_x_train, y_train_encoded), 1):\n",
        "    print(f\"\\nFold {fold}/10\")\n",
        "\n",
        "    x_train_fold = [prep_x_train[i] for i in train_idx]\n",
        "    x_val_fold = [prep_x_train[i] for i in val_idx]\n",
        "    y_train_fold = y_train_encoded[train_idx]\n",
        "    y_val_fold = y_train_encoded[val_idx]\n",
        "\n",
        "    tfidf_fold = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2,\n",
        "        max_df=0.90,\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    x_train_tfidf = tfidf_fold.fit_transform(x_train_fold)\n",
        "    x_val_tfidf = tfidf_fold.transform(x_val_fold)\n",
        "\n",
        "    svm = LinearSVC(\n",
        "        class_weight=\"balanced\", # balanced mode uses value of y to adjust weights\n",
        "        max_iter=1000,\n",
        "        C=1.0\n",
        "    )\n",
        "\n",
        "    svm.fit(x_train_tfidf, y_train_fold)\n",
        "    svm_pred = svm.predict(x_val_tfidf)\n",
        "\n",
        "    svm_acc = accuracy_score(y_val_fold, svm_pred)\n",
        "    svm_f1_macro = f1_score(y_val_fold, svm_pred, average='macro')\n",
        "\n",
        "    svm_acc_scores.append(svm_acc)\n",
        "    svm_f1_macro_scores.append(svm_f1_macro)\n",
        "\n",
        "    print(f\"  LinearSVC Accuracy: {svm_acc:.4f}, F1-macro: {svm_f1_macro:.4f}\")\n",
        "\n",
        "    mnb = MultinomialNB(alpha=1.0)\n",
        "    mnb.fit(x_train_tfidf, y_train_fold)\n",
        "    mnb_pred = mnb.predict(x_val_tfidf)\n",
        "\n",
        "    mnb_acc = accuracy_score(y_val_fold, mnb_pred)\n",
        "    mnb_f1_macro = f1_score(y_val_fold, mnb_pred, average='macro')\n",
        "\n",
        "    mnb_acc_scores.append(mnb_acc)\n",
        "    mnb_f1_macro_scores.append(mnb_f1_macro)\n",
        "\n",
        "    print(f\"  MultinomialNB Accuracy: {mnb_acc:.4f}, F1-macro: {mnb_f1_macro:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66fFR842LKDO",
        "outputId": "51754502-fbc3-4823-896c-b7caf5c09808"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/10\n",
            "  LinearSVC Accuracy: 0.4502, F1-macro: 0.3315\n",
            "  MultinomialNB Accuracy: 0.4502, F1-macro: 0.1632\n",
            "\n",
            "Fold 2/10\n",
            "  LinearSVC Accuracy: 0.4373, F1-macro: 0.3313\n",
            "  MultinomialNB Accuracy: 0.4405, F1-macro: 0.1551\n",
            "\n",
            "Fold 3/10\n",
            "  LinearSVC Accuracy: 0.4791, F1-macro: 0.3786\n",
            "  MultinomialNB Accuracy: 0.4341, F1-macro: 0.1400\n",
            "\n",
            "Fold 4/10\n",
            "  LinearSVC Accuracy: 0.4212, F1-macro: 0.2873\n",
            "  MultinomialNB Accuracy: 0.4437, F1-macro: 0.1456\n",
            "\n",
            "Fold 5/10\n",
            "  LinearSVC Accuracy: 0.4920, F1-macro: 0.4246\n",
            "  MultinomialNB Accuracy: 0.4341, F1-macro: 0.1556\n",
            "\n",
            "Fold 6/10\n",
            "  LinearSVC Accuracy: 0.4566, F1-macro: 0.3796\n",
            "  MultinomialNB Accuracy: 0.4502, F1-macro: 0.1605\n",
            "\n",
            "Fold 7/10\n",
            "  LinearSVC Accuracy: 0.4695, F1-macro: 0.3812\n",
            "  MultinomialNB Accuracy: 0.4598, F1-macro: 0.1707\n",
            "\n",
            "Fold 8/10\n",
            "  LinearSVC Accuracy: 0.4710, F1-macro: 0.3652\n",
            "  MultinomialNB Accuracy: 0.4258, F1-macro: 0.1402\n",
            "\n",
            "Fold 9/10\n",
            "  LinearSVC Accuracy: 0.4000, F1-macro: 0.2970\n",
            "  MultinomialNB Accuracy: 0.4323, F1-macro: 0.1396\n",
            "\n",
            "Fold 10/10\n",
            "  LinearSVC Accuracy: 0.4258, F1-macro: 0.3492\n",
            "  MultinomialNB Accuracy: 0.4387, F1-macro: 0.1520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4 - Evaluation"
      ],
      "metadata": {
        "id": "5SXJoPQzyVUR"
      }
    }
  ]
}